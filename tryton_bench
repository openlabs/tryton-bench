#!/bin/env python
# -*- coding: utf-8 -*-
"""
    tryton_bench

    Stress test the server with several concurrent requests

    :copyright: (c) 2013 by Openlabs Technologies & Consulting (P) Limited
    :license: Modified BSD, see LICENSE for more details.
"""


import argparse
import json
import operator
import os
import requests
import sys
import time
import zlib
from multiprocessing import Process
from multiprocessing import Queue
from requests import RequestException


class HttpClient:
    """HTTP Client to make JSON RPC requests to Tryton server.
    User is logged in when an object is created.
    """
    def __init__(self, url, database_name, user, passwd):
        self._url = '{}/{}'.format(url, database_name)
        self._user = user
        self._passwd = passwd
        self._login()

    def _login(self):
        payload = json.dumps({
            'params': [self._user, self._passwd],
            'jsonrpc': "2.0",
            'method': 'common.login',
            'id': None
        })
        result = requests.post(self._url, payload)
        if 'json' in result:
            self._session = result.json()['result']
        else:
            self._session = json.loads(result.text)['result']
        return self._session

    def call(self, model, method, *args):
        """RPC Call
        """
        method = '{}.{}.{}'.format('model', model, method)
        payload = json.dumps({
            'params': [
                self._session[0],
                self._session[1],
            ] + list(args) + [{}],
            'method': method,
            'id': None
        })
        return requests.post(self._url, payload).status_code


class Scenario:
    """Takes a module name, loads it and loads the test case
    defined in it, as well as generate datasets.
    """
    def __init__(self, module_name):
        self.module = __import__(module_name)

    def model(self):
        return self.module.MODEL

    def method(self):
        return self.module.METHOD

    def generate(self):
        return self.module.generate()


class Lap:
    def __init__(self):
        self.data = [time.time()]

    def note(self):
        self.data.append(time.time())

    def last(self):
        return self.data[-1] - self.data[-2]

    def elapsed(self):
        return self.data[-1] - self.data[0]


#NOTE: This has disadvantage of losing all progress stats made by worker.
#      But for a very little while, lets live with that.
#TODO: Let progress queue also send number of successful and failed stats, so
#      that we don't lose information.
def gracefully(func):
    def push_to_queue(*args, **kwargs):
        completed_q = kwargs.get('completed_q', args[-1])
        progress_q = kwargs.get('progress_q', args[-2])
        num = kwargs.get('num', args[2])
        completed_q.put((num, 0, 0))
        progress_q.put(0)

    def f(*args, **kwargs):
        result = 0
        try:
            result = func(*args, **kwargs)
        except KeyboardInterrupt:
            print "KeyboaradInterrup: Worker shutting down"
            push_to_queue(*args, **kwargs)
        except RequestException:
            print "Worker shutting down. Error making requests."
            push_to_queue(*args, **kwargs)
        except Exception as e:
            print "Worker crashed! ", e.message
            push_to_queue(*args, **kwargs)
        return result
    return f


@gracefully
def blast_server(env, scenario_name, num, progress_q, completed_q):
    """Worker method to make requests serially
    """
    INTERVAL = num // 10
    scenario = Scenario(scenario_name)
    client = HttpClient(env.url, env.database, env.user, env.password)
    good, bad = 0, 0
    for i in range(num):
        data = scenario.generate()
        status_code = client.call(scenario.model(), scenario.method(),
                *data.get('args', []), **data.get('kwargs', {}))
        if status_code >= 400 and status_code < 600:
            bad += 1
        else:
            good += 1
        if i % INTERVAL == 0:
            progress_q.put(INTERVAL)
    progress_q.put(num % INTERVAL)
    completed_q.put((num, good, bad))
    return num


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--url', default='localhost',
            help='URL of the JSON RPC server')
    parser.add_argument('-d', '--database', required=True,
            help='Tryton database')
    parser.add_argument('-u', '--user', required=True, help='Tryton user')
    parser.add_argument('-p', '--password', required=True,
            help='Tryton password')
    parser.add_argument('-r', '--requests', type=int, required=True,
            help='Number of requests')
    parser.add_argument('-c', '--connections', type=int, required=True,
            help='Number of concurrent connections')
    parser.add_argument('-s', '--scenario', required=True,
            help='Module which will generate test cases')
    args = parser.parse_args()

    return args


def main():
    args = parse_args()

    print "TrytonBench: Stress test JSON RPC Server"
    print "----------------------------------------\n"
    print "  Target                 :  {url}/{db}".format(
            url=args.url, db=args.database)
    print "  Requests to serve      :  {}".format(args.requests)
    print "  Concurrent connections :  {}".format(args.connections)
    print "\n"

    # divide task almost equally among workers
    chunk_size = args.requests // args.connections
    pool = []

    # Progress keep tracks of number of requests served til now by worker. 
    # Completed is used as a kind of semaphore for the loop for polling
    # progress.
    progress, completed = Queue(), Queue(args.connections)

    lap = Lap()
    for i in range(args.connections):
        #sender, receiver = Pipe()
        #print receiver
        if i == args.connections - 1:
            chunk_size = args.requests - (chunk_size * i)
        p = Process(target=blast_server,
                args=(args, args.scenario, chunk_size, progress, completed))
        pool.append(p)
        p.start()

    # track progress
    total = 0
    last_percentage, percentage = 0, 0
    print "  Percentage of requests served - "
    while not completed.full():
        total += progress.get()
        percentage = int(float(100 * total) / args.requests)
        if percentage - last_percentage >= 10:
            last_percentage = percentage
            lap.note()
            print "   {:{w1}d}%  ---  {:0.2f} seconds".format(
                    percentage, lap.elapsed(), w1=4)

    # all workers are finished now, but we still gotta join
    aggregate = [0, 0, 0]  # [num, good, bad]
    for i in range(args.connections):
        pool[i].join()
        aggregate = map(operator.add, aggregate, completed.get())

    print "\n"
    print "  Successful requests    :  {}".format(aggregate[1])
    print "  Failed requests        :  {}".format(aggregate[2])
    print "  Total time taken       :  {:0.2f} seconds".format(lap.elapsed())


if __name__ == '__main__':
    sys.path.append(os.getcwd())
    main()
